{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Based Neural Network main run \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pyarrow\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "\n",
    "# 3rd party packages\n",
    "#from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial\n",
    "from running import UnsupervisedRunner\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_df = 4\n",
    "list_temp = ['columns'] #,'actsoc_ep1_x_b3','ambientairtemperature','actpackvoltage_ep1_x_b3',\n",
    "list_std = ['columns'] # list to give for standardization inside running, without the diff column\n",
    "l2_reg = 1\n",
    "lr = 0.0001\n",
    "max_len = 1024\n",
    "schedule_factor = 0.5\n",
    "sch_patience = 3\n",
    "mask_len = 5\n",
    "mask_ratio = 0.2\n",
    "folder_name = \"results_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder creation for the necessary files to be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{folder_name}/eval_metrics\")\n",
    "os.makedirs(f\"{folder_name}/eval_metrics/predplot_plotly\")\n",
    "writer = pd.ExcelWriter(f\"{folder_name}/eval_metrics/{folder_name}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = r'path_to_import_data'\n",
    "\n",
    "df_pd = pd.read_parquet(parquet_file,engine='pyarrow')\n",
    "df_pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing and dropping some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_pd.drop(columns=['time_abs'],axis=1)\n",
    "list1 = ['all_required_cols']\n",
    "\n",
    "list_col = ['columns_to_consider']\n",
    "\n",
    "#list_temp = ['ambientairtemperature','actpackvoltage_ep1_x_b3','actcurrent_ep1_x_b3','actsoc_ep1_x_b3','time_diff']\n",
    "\n",
    "df = df_pd[list_col]\n",
    "\n",
    "df = df.rename(columns={'a': 'b'})\n",
    "df = df[df['b'] < 600]\n",
    "df = df.drop(columns=['b'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file1 = r'standardized_data'\n",
    "\n",
    "filtered_df = pd.read_parquet(parquet_file1,engine='pyarrow')\n",
    "\n",
    "parquet_file2 = r'original data'\n",
    "\n",
    "org_df = pd.read_parquet(parquet_file2,engine='pyarrow')\n",
    "\n",
    "list_t = ['columns to consider for prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split for train,val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = list(df.index.unique())\n",
    "random.shuffle(n_batch)\n",
    "train = int(np.round(0.7*len(n_batch)))\n",
    "val = train + int(np.round(0.2*len(n_batch)))\n",
    "test = val + int(np.round(0.1*len(n_batch)))\n",
    "\n",
    "train_df = df.loc[n_batch[:train]]\n",
    "val_df = df.loc[n_batch[train+1:val]]\n",
    "test_df = df.loc[n_batch[val+1:test]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(5)\n",
    "# pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#te = df.groupby(df.index)\n",
    "#a = te.size()\n",
    "#print(a.idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #c_batch = df.pop('b')\n",
    "# c_batch = pd.Series(df.pop('b'),dtype='int32')\n",
    "# c_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# plt.plot(exp_var_ratio)\n",
    "# plt.xlabel('Number of components')\n",
    "# plt.ylabel('Explained Variance Ratio')\n",
    "# plt.show()\n",
    "\n",
    "# cum_exp_var_ratio = np.cumsum(exp_var_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pca = pca.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pca =  pd.DataFrame(df_pca)\n",
    "# df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.concat([df_pd, df_pca], axis=1)\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'df_pca['b'] = c_batch\n",
    "# #df_pca = df_pca.set_index('b')\n",
    "# df_pca'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImputationDataset(Dataset):\n",
    "    \"\"\"Dynamically computes missingness (noise) mask for each sample\"\"\"\n",
    "\n",
    "    def __init__(self, df, indices, mean_mask_length=mask_len, masking_ratio=mask_ratio,  #mean_mask_len = 3\n",
    "                 mode='separate', distribution='geometric', exclude_feats=None):\n",
    "        super(ImputationDataset, self).__init__()\n",
    "\n",
    "        self.data = df  # this is a subclass of the BaseData class in data.py\n",
    "        self.IDs = indices  # list of data IDs, but also mapping between integer index and ID\n",
    "        self.feature_df = self.data.loc[self.IDs]\n",
    "\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mean_mask_length = mean_mask_length\n",
    "        self.mode = mode\n",
    "        self.distribution = distribution\n",
    "        self.exclude_feats = exclude_feats\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        \"\"\"\n",
    "        For a given integer index, returns the corresponding (seq_length, feat_dim) array and a noise mask of same shape\n",
    "        Args:\n",
    "            ind: integer index of sample in dataset\n",
    "        Returns:\n",
    "            X: (seq_length, feat_dim) tensor of the multivariate time series corresponding to a sample\n",
    "            mask: (seq_length, feat_dim) boolean tensor: 0s mask and predict, 1s: unaffected input\n",
    "            ID: ID of sample\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.feature_df.loc[self.IDs[ind]].values  # (seq_length, feat_dim) array # IDs[ind]\n",
    "        mask = noise_mask(X, self.masking_ratio, self.mean_mask_length, self.mode, self.distribution,\n",
    "                          self.exclude_feats)  # (seq_length, feat_dim) boolean array\n",
    "        #print(\"aa\",self.data.shape[0])\n",
    "        return torch.from_numpy(X), torch.from_numpy(mask), self.IDs[ind]\n",
    "\n",
    "    def update(self):\n",
    "        self.mean_mask_length = min(20, self.mean_mask_length + 1)\n",
    "        self.masking_ratio = min(1, self.masking_ratio + 0.05)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Mask Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(X, masking_ratio, lm=mask_len, mode='separate', distribution='geometric', exclude_feats=None):\n",
    "    \"\"\"\n",
    "    Creates a random boolean mask of the same shape as X, with 0s at places where a feature should be masked.\n",
    "    Args:\n",
    "        X: (seq_length, feat_dim) numpy array of features corresponding to a single sample\n",
    "        masking_ratio: proportion of seq_length to be masked. At each time step, will also be the proportion of\n",
    "            feat_dim that will be masked on average\n",
    "        lm: average length of masking subsequences (streaks of 0s). Used only when `distribution` is 'geometric'.\n",
    "        mode: whether each variable should be masked separately ('separate'), or all variables at a certain positions\n",
    "            should be masked concurrently ('concurrent')\n",
    "        distribution: whether each mask sequence element is sampled independently at random, or whether\n",
    "            sampling follows a markov chain (and thus is stateful), resulting in geometric distributions of\n",
    "            masked squences of a desired mean length `lm`\n",
    "        exclude_feats: iterable of indices corresponding to features to be excluded from masking (i.e. to remain all 1s)\n",
    "    Returns:\n",
    "        boolean numpy array with the same shape as X, with 0s at places where a feature should be masked\n",
    "    \"\"\"\n",
    "    if exclude_feats is not None:\n",
    "        exclude_feats = set(exclude_feats)\n",
    "\n",
    "    if distribution == 'geometric':  # stateful (Markov chain)\n",
    "        if mode == 'separate':  # each variable (feature) is independent\n",
    "            mask = np.ones(X.shape, dtype=bool)\n",
    "            #print(mask.shape)\n",
    "            for m in range(X.shape[1]):  # feature dimension\n",
    "                if exclude_feats is None or m not in exclude_feats:\n",
    "                    mask[:, m] = geom_noise_mask_single(X.shape[0], lm, masking_ratio)  # time dimension\n",
    "        \n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_noise_mask_single(L, lm, masking_ratio):\n",
    "    \"\"\"\n",
    "    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n",
    "    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n",
    "    Args:\n",
    "        L: length of mask and sequence to be masked\n",
    "        lm: average length of masking subsequences (streaks of 0s)\n",
    "        masking_ratio: proportion of L to be masked\n",
    "    Returns:\n",
    "        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n",
    "    \"\"\"\n",
    "    keep_mask = np.ones(L, dtype=bool)\n",
    "    p_m = 1 / lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n",
    "    p_u = p_m * masking_ratio / (1 - masking_ratio)  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n",
    "    p = [p_m, p_u]\n",
    "\n",
    "    # Start in state 0 with masking_ratio probability\n",
    "    state = int(np.random.rand() > masking_ratio)  # state 0 means masking, 1 means not masking\n",
    "    for i in range(L):\n",
    "        keep_mask[i] = state  # here it happens that state and masking value corresponding to state are identical\n",
    "        if np.random.rand() < p[state]:\n",
    "            state = 1 - state\n",
    "\n",
    "    return keep_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model, Optimizer and Loss Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import TSTransformerEncoder\n",
    "model = TSTransformerEncoder(column_df,max_len,['give_the_required_values_from_function'],pos_encoding='learnable',activation='gelu',norm='BatchNorm',freeze=False)\n",
    "\n",
    "from loss import MaskedMSELoss\n",
    "loss_module = MaskedMSELoss()\n",
    "\n",
    "from torch.optim.optimizer import Optimizer \n",
    "from optimizer import RAdam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "optimizer = RAdam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=schedule_factor, patience=sch_patience, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_unsuperv(data, max_len=None, mask_compensation=False):\n",
    "    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n",
    "    Args:\n",
    "        data: len(batch_size) list of tuples (X, mask).\n",
    "            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "            - mask: boolean torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n",
    "            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n",
    "    Returns:\n",
    "        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n",
    "        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n",
    "        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n",
    "            0 indicates masked values to be predicted, 1 indicates unaffected/\"active\" feature values\n",
    "        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 ignore (padding)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(data)\n",
    "    features, masks,IDs = zip(*data)\n",
    "    idx_dict = { '': { }, \n",
    "         '': { }}\n",
    "    lengths = [X.shape[0] for X in features] \n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "    X = torch.zeros(batch_size, max_len, features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n",
    "    target_masks = torch.zeros_like(X,\n",
    "                                    dtype=torch.bool)  # (batch_size, padded_length, feat_dim) masks related to objective\n",
    "    idx_arr = []\n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        if lengths[i] > max_len:\n",
    "            sl = random.randint(0,(lengths[i]-max_len))\n",
    "            slen = min(lengths[i] - sl, max_len)\n",
    "            X[i, :slen, :] = features[i][sl:sl+slen, :]\n",
    "            target_masks[i, :slen, :] = masks[i][sl:sl+slen, :]\n",
    "            idx = slen\n",
    "            \n",
    "        else:\n",
    "            X[i,:end,:] = features[i][:end, :]\n",
    "            target_masks[i, :end, :] = masks[i][:end, :]\n",
    "            idx = 0\n",
    "        \n",
    "            \n",
    "    targets = X.clone()\n",
    "    X = X * target_masks  # mask input ##change\n",
    "    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.float32), max_len=max_len)  # (batch_size, padded_length) boolean tensor, \"1\" means keep\n",
    "    target_masks = ~target_masks  # inverse logic: 0 now means ignore, 1 means predict\n",
    "    \n",
    "    return X.to(device), targets.to(device), target_masks.to(device), padding_masks.to(device),IDs,idx\n",
    "\n",
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n",
    "    where 1 means keep element at this position (time step)\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max_val()  # trick works because of overloading of 'or' operator for non-boolean types\n",
    "    return (torch.arange(0, max_len)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, epoch, model, optimizer=None):\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        state_dict = model.state_dict()\n",
    "    data = {'epoch': epoch,\n",
    "            'state_dict': state_dict}\n",
    "    if not (optimizer is None):\n",
    "        data['optimizer'] = optimizer.state_dict()\n",
    "    torch.save(data, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_path, optimizer=None, resume=False, change_output=False,\n",
    "               lr=None, lr_step=None, lr_factor=None):\n",
    "    start_epoch = 0\n",
    "    checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    state_dict = deepcopy(checkpoint['state_dict'])\n",
    "    if change_output:\n",
    "        for key, val in checkpoint['state_dict'].items():\n",
    "            if key.startswith('output_layer'):\n",
    "                state_dict.pop(key)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    print('Loaded model from {}. Epoch: {}'.format(model_path, checkpoint['epoch']))\n",
    "\n",
    "    # resume optimizer parameters\n",
    "    if optimizer is not None and resume:\n",
    "        if 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            start_lr = lr\n",
    "            for i in range(len(lr_step)):\n",
    "                if start_epoch >= lr_step[i]:\n",
    "                    start_lr *= lr_factor[i]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = start_lr\n",
    "            print('Resumed optimizer with start lr', start_lr)\n",
    "        else:\n",
    "            print('No optimizer parameters in checkpoint.')\n",
    "    if optimizer is not None:\n",
    "        return model, optimizer, start_epoch\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_factory():\n",
    "    \"\"\"For the task specified in the configuration returns the corresponding combination of\n",
    "    Dataset class, collate function and Runner class.\"\"\"\n",
    "    \n",
    "    return ImputationDataset, collate_unsuperv, UnsupervisedRunner\n",
    "\n",
    "dataset_class, collate_fn, runner_class = pipeline_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df.index.unique()\n",
    "\n",
    "train_dataset = dataset_class(train_df, a,mask_len,mask_ratio)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=your_wish, \n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: collate_unsuperv(x, max_len=model.max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = val_df.index.unique()\n",
    "\n",
    "val_dataset = dataset_class(val_df, b,mask_len,mask_ratio)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                         batch_size=your_wish,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: collate_unsuperv(x, max_len=model.max_len)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "trainer = UnsupervisedRunner(model, train_loader, device, loss_module,org_df,folder_name,list_std,optimizer,l2_reg=l2_reg)\n",
    "validator = UnsupervisedRunner(model, val_loader, device, loss_module,org_df,folder_name,list_std,optimizer,scheduler,l2_reg=l2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "no_of_epochs = 'for_iterations'\n",
    "pt_epoch = []\n",
    "pt_loss = []\n",
    "vl_loss = []\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "vl_loss_best = 'threshold'\n",
    "best_epoch = -1\n",
    "\n",
    "dict_pt = {}\n",
    "if not os.path.exists(f\"path_for_training_models\"):\n",
    "    os.makedirs(f\"path_for_training_models\")\n",
    "epoch_time = []\n",
    "path = f\"path_for_training_models\"\n",
    "for epochs in range(no_of_epochs):\n",
    "    start = time.time()\n",
    "    a,b,c = trainer.train_epoch(epochs)\n",
    "    d,e = validator.evaluate(epochs)\n",
    "    #if (epochs % 10 == 0):\n",
    "    #if b < 10:5\n",
    "    #dict_pt[f\"epoch_{epochs}\"] = c\n",
    "    vl_loss.append(e)\n",
    "    pt_epoch.append(a)\n",
    "    pt_loss.append(b)\n",
    "    \n",
    "    if e < best_val_loss:\n",
    "        best_val_loss = e\n",
    "        counter = 0\n",
    "        save_model(path+'epoch_{}'.format(epochs), epochs, model, optimizer)\n",
    "    else:\n",
    "        counter += 1\n",
    "    #print(best_val_loss)\n",
    "    # Stop training if the validation loss has not improved for a certain number of epochs\n",
    "    if counter >= patience:\n",
    "        print(f'Early stopping after {epochs} epochs')\n",
    "        end = time.time()\n",
    "        epoch_t = end - start\n",
    "        epoch_time.append(epoch_t)\n",
    "        break\n",
    "    \n",
    "#     if vl_loss[epochs] < vl_thres:\n",
    "#         vl_loss_best = vl_loss[epochs]\n",
    "#         best_epoch = epochs \n",
    "    \n",
    "    #save_model(path+'epoch_{}'.format(epochs), epochs, model, optimizer)\n",
    "    end = time.time()\n",
    "    epoch_t = end - start\n",
    "    epoch_time.append(epoch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the minimum training and validation losses epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_val_pt = pt_loss.index(min(pt_loss))\n",
    "epoch_valid_pt = vl_loss.index(min(vl_loss))\n",
    "\n",
    "print(f\"Minimum Training loss at {epoch_val_pt} epoch \\n Minimum Validation loss at {epoch_valid_pt} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the metrics to an excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_df = pd.DataFrame({\"epochs\":pt_epoch,\"validation_loss\":vl_loss,\"training_loss\":pt_loss,\"time_for_epoch\":epoch_time})\n",
    "epoch_df.to_excel(writer, sheet_name='tr_val_metrics', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the epoch of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_valid_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.suptitle('Training and valiation loss per epoch', fontsize=15)\n",
    "plt.xlabel('epochs', fontsize=10, rotation='horizontal')\n",
    "#plt.ylim([0,0.1])\n",
    "plt.plot(pt_epoch,pt_loss)\n",
    "plt.plot(pt_epoch,vl_loss)\n",
    "plt.legend([\"Training loss\", \"Validation loss\"], loc =\"upper right\")\n",
    "plt.savefig(f\"/mnt/proj/emob-da1/zz_thesis_work/Thesis - Copy/{folder_name}/eval_metrics/train_val_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the best model from the training run and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, start_epoch = load_model(model, model_path =f\"path_to_save_best_epoch_model\",   #{epoch_valid_pt}\n",
    "                                           optimizer = optimizer ,\n",
    "                                           resume=False ,\n",
    "                                           change_output=False ,\n",
    "                                           lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = test_df.index.unique()\n",
    "\n",
    "test_dataset = dataset_class(test_df, c,mask_len,mask_ratio)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size= 'your_wish', \n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: collate_unsuperv(x, max_len=model.max_len)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['the_cols_you_want']\n",
    "device = torch.device(\"cuda\")\n",
    "tester = UnsupervisedRunner(model, test_loader, device, loss_module,org_df,folder_name,list_std,optimizer,l2_reg=l2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_epochs = 1\n",
    "test_epoch = []\n",
    "test_loss = []\n",
    "epoch_time = []\n",
    "rmse_dict = dict()\n",
    "mae_dict = dict()\n",
    "for epochs in range(no_of_epochs):\n",
    "    start = time.time()\n",
    "    temp_a,b,c,d = tester.test_epoch(epochs)\n",
    "    test_epoch.append(temp_a)\n",
    "    test_loss.append(b)\n",
    "    \n",
    "    rmse_dict[f\"epoch_path\"] = c\n",
    "    mae_dict[f\"epoch_path\"] = d\n",
    "    \n",
    "    end = time.time()\n",
    "    epoch_t = end - start\n",
    "    epoch_time.append(epoch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the test losses, rmse and mae metrics to an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(rmse_dict)\n",
    "dd['idx'] = test_df.index.unique()\n",
    "dd = dd.set_index('idx')\n",
    "dd['batch_rmse'] = dd.mean(axis=1)\n",
    "\n",
    "ee = pd.DataFrame(mae_dict)\n",
    "ee['idx'] = test_df.index.unique()\n",
    "ee = ee.set_index('idx')\n",
    "ee['batch_mae'] = ee.mean(axis=1)\n",
    "\n",
    "test_ex = pd.DataFrame({\"epochs\":test_epoch,\"test_loss\":test_loss,\"time_for_epoch\":epoch_time})\n",
    "test_ex.to_excel(writer, sheet_name='test_metrics', index=False)\n",
    "\n",
    "dd.to_excel(writer, sheet_name='rmse', index=False)\n",
    "ee.to_excel(writer, sheet_name='mae', index=False)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.suptitle('Test loss per epoch', fontsize=15)\n",
    "plt.xlabel('epochs', fontsize=10, rotation='horizontal')\n",
    "#plt.ylim([0,0.1])\n",
    "plt.plot(test_epoch,test_loss)\n",
    "plt.savefig(f\"/path_to_save_file\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org = the original file\n",
    "df_pred = the predicted file\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 8))\n",
    "\n",
    "columns = ['ambientairtemperature','actsoc_ep1_x_b3','actpackvoltage_ep1_x_b3',\n",
    "'actcurrent_ep1_x_b3'\n",
    "]    #         \n",
    "\n",
    "df_vstack_org = df_org.iloc[:100]\n",
    "df_vstack_pred = df_pred.iloc[:100]\n",
    "\n",
    "# diff = df_vstack_org - df_vstack_pred\n",
    "\n",
    "# mask = diff > 0\n",
    "\n",
    "for i in range(4):\n",
    "    min_org = min(df_vstack_org[columns[i]])\n",
    "    min_pred = min(df_vstack_pred[columns[i]])\n",
    "    max_org = max(df_vstack_org[columns[i]])\n",
    "    max_pred = max(df_vstack_pred[columns[i]])\n",
    "    \n",
    "    fin_min = min(min_org,min_pred)\n",
    "    fin_max = max(max_org,max_pred)\n",
    "    for j in range(2):\n",
    "# Plot each feature vertically\n",
    "        if j == 0:\n",
    "            axes[i,j].plot(df_vstack_org.index, df_vstack_org[columns[i]],color = 'grey', label = 'aa')\n",
    "            \n",
    "            axes[i,j].scatter(df_vstack_org[columns[i]][df_mask[columns[i]]].index, df_vstack_org[columns[i]][df_mask[columns[i]]],color = 'blue', label = 'bb')\n",
    "            axes[i,j].set_ylabel(f\"{columns[i]}\")\n",
    "            axes[i,j].set_title(f\"Original plot of {columns[i]}\")\n",
    "            axes[i,j].set_ylim(fin_min,fin_max)\n",
    "            \n",
    "        if j == 1:\n",
    "            axes[i,j].plot(df_vstack_pred.index, df_vstack_pred[columns[i]],color = 'grey', label = 'aa')\n",
    "            axes[i,j].scatter(df_vstack_pred[columns[i]][df_mask[columns[i]]].index, df_vstack_pred[columns[i]][df_mask[columns[i]]],color = 'green', label = 'aa')\n",
    "            axes[i,j].set_ylabel(f\"{columns[i]}\")\n",
    "            axes[i,j].set_title(f\"Predicted plot of {columns[i]}\")\n",
    "            axes[i,j].set_ylim(fin_min,fin_max)\n",
    "        \n",
    "        axes[-1,j].set_xlabel('Time')\n",
    "\n",
    "# Set common x-axis label\n",
    "\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fc46926825c2aee23059f78dd3e0fa551d7dca4cc5c295581ff4ff4427cc0b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
